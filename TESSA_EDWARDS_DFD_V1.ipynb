{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1vFaE0BmyykpAUpDm0U_GLZhelvyExfV0",
      "authorship_tag": "ABX9TyNyVerjcapm0/jnqTrUk2wg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeadtommiVR/DF-Detection-Exploration-/blob/main/TESSA_EDWARDS_DFD_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGtb7gZ2Ty5s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Project Overview**\n",
        "This project focuses on detecting deepfake videos by analyzing **spatial artifacts (within frames) and temporal inconsistencies (across frames)**. Since deepfake generation struggles to accurately replicate **natural motion**, our approach leverages **optical flow analysis, CNN-based feature extraction, and LSTM for temporal modeling**.\n",
        "\n",
        "# **Deepfake Detection Dataset Processing & Preparation**\n",
        "### **What I Did**\n",
        "Given time and compute constraints, we:\n",
        "- **Used a subset of videos** instead of the full dataset.\n",
        "- **Extracted frames** to analyze sequences instead of raw video files.\n",
        "- **Standardized sequences** so all videos have the same frame count.\n",
        "- **Used a CNN + LSTM pipeline** instead of a full 3D convolutional model.\n",
        "\n",
        "### **What I Would Have Done with More Compute**\n",
        "- **Used 3D CNNs (e.g., I3D, C3D, or SlowFast)** to model spatio-temporal patterns without needing separate feature extraction.\n",
        "- **Applied self-supervised learning** for unsupervised deepfake feature discovery.\n",
        "- **Implemented transformer-based architectures (e.g., ViViT, TimeSformer)** for direct sequence modeling.\n",
        "\n",
        "---\n",
        "\n",
        "# **Deepfake Detection Dataset Handling Notes**\n",
        "\n",
        "## **1. Downloading the Dataset**\n",
        "The dataset consists of **high-resolution video files**, making direct processing difficult. Instead of working with the entire dataset, we:\n",
        "- **Downloaded a subset** from the original dataset source.\n",
        "- **Stored it on an external hard drive** to manage storage constraints.\n",
        "- **Uploaded a smaller working subset to Google Drive** for processing in Google Colab.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Inspecting the Data**\n",
        "Before using the dataset, we performed an initial inspection to:\n",
        "- **Verify file formats** (ensure all files are valid `.mp4` videos).\n",
        "- **Detect corrupted files** (videos that fail to open or have broken frames).\n",
        "- **Remove mislabeled or unnecessary metadata** to avoid bias.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Cleaning the Dataset**\n",
        "To **prevent data leakage** and ensure a fair evaluation, we:\n",
        "- **Removed corrupted videos** that could not be processed.\n",
        "- **Anonymized filenames** to prevent the model from learning patterns from names instead of content.\n",
        "- **Shuffled the dataset** to avoid structured order bias in training.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Extracting Frames from Videos**\n",
        "Since raw videos cannot be directly processed in most machine learning models, we:\n",
        "- **Converted each video into a sequence of image frames**.\n",
        "- **Ensured frames were extracted at equal intervals** (to preserve temporal consistency).\n",
        "- **Organized extracted frames into structured folders**, maintaining a distinction between real and fake videos.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Standardizing Frame Sequences**\n",
        "Videos in the dataset varied in length, which would cause inconsistencies in training. To address this:\n",
        "- We **identified the video with the fewest frames** in our dataset.\n",
        "- We **trimmed all other videos** to match this sequence length.\n",
        "- This step ensured **all input sequences were of equal length**, preventing bias in model training.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Uploading to Colab**\n",
        "To efficiently handle dataset processing, we:\n",
        "- **Uploaded the selected subset to Google Drive** to enable access from Colab.\n",
        "- **Mounted Google Drive in Colab** to avoid memory limitations.\n",
        "- **Structured the dataset in a clean format** (`DFD_REAL` and `DFD_FAKE` folders).\n",
        "\n",
        "---\n",
        "\n",
        "## **Next Steps**\n",
        "With the dataset now fully processed, we are ready to:\n",
        "1. **Extract Optical Flow features** to detect motion inconsistencies.\n",
        "2. **Train a CNN (ResNet/EfficientNet) to analyze spatial artifacts in frames**.\n",
        "3. **Feed CNN features into an LSTM** to learn temporal inconsistencies.\n",
        "4. **Optimize the model using Bayesian Hyperparameter Tuning**.\n",
        "\n",
        "This structured pipeline ensures that our deepfake detection approach **focuses on motion inconsistencies**, an area where deepfake models struggle the most.\n",
        "\n"
      ],
      "metadata": {
        "id": "gs0T36opXHqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09QAupFPMFNe",
        "outputId": "2a25e950-57a0-49d6-a86c-c4ffa9569342"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Define paths to real and fake video folders in Google Drive\n",
        "real_videos_path = \"/content/drive/My Drive/DFD_REAL\"\n",
        "fake_videos_path = \"/content/drive/My Drive/DFD_FAKE\"\n",
        "\n",
        "# Get list of video files in each folder\n",
        "real_videos = sorted(glob(os.path.join(real_videos_path, \"*.mp4\")))\n",
        "fake_videos = sorted(glob(os.path.join(fake_videos_path, \"*.mp4\")))\n",
        "\n",
        "# Print counts and sample files\n",
        "print(f\" Found {len(real_videos)} real videos\")\n",
        "print(f\" Found {len(fake_videos)} fake videos\")\n",
        "print(\"Sample real video:\", real_videos[:2])\n",
        "print(\"Sample fake video:\", fake_videos[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkJhhYAlqsQI",
        "outputId": "c4b27420-f5b0-46fb-9cca-3fca43ad58b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Found 16 real videos\n",
            " Found 16 fake videos\n",
            "Sample real video: ['/content/drive/My Drive/DFD_REAL/01__exit_phone_room.mp4', '/content/drive/My Drive/DFD_REAL/01__hugging_happy.mp4']\n",
            "Sample fake video: ['/content/drive/My Drive/DFD_FAKE/07_02__outside_talking_pan_laughing__1ZE4HC06.mp4', '/content/drive/My Drive/DFD_FAKE/07_02__outside_talking_pan_laughing__O4SXNLRL.mp4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "# Define paths to real and fake video folders in Google Drive\n",
        "real_videos_path = \"/content/drive/My Drive/DFD_REAL\"\n",
        "fake_videos_path = \"/content/drive/My Drive/DFD_FAKE\"\n",
        "\n",
        "# Get list of video files in each folder\n",
        "real_videos = sorted([f for f in os.listdir(real_videos_path) if f.endswith(\".mp4\")])\n",
        "fake_videos = sorted([f for f in os.listdir(fake_videos_path) if f.endswith(\".mp4\")])\n",
        "\n",
        "# Shuffle files to remove any order bias\n",
        "random.shuffle(real_videos)\n",
        "random.shuffle(fake_videos)\n",
        "\n",
        "# Rename real videos\n",
        "for i, filename in enumerate(real_videos):\n",
        "    old_path = os.path.join(real_videos_path, filename)\n",
        "    new_filename = f\"video_{i:03d}.mp4\"\n",
        "    new_path = os.path.join(real_videos_path, new_filename)\n",
        "    os.rename(old_path, new_path)\n",
        "\n",
        "# Rename fake videos\n",
        "for i, filename in enumerate(fake_videos):\n",
        "    old_path = os.path.join(fake_videos_path, filename)\n",
        "    new_filename = f\"video_{i + len(real_videos):03d}.mp4\"\n",
        "    new_path = os.path.join(fake_videos_path, new_filename)\n",
        "    os.rename(old_path, new_path)\n",
        "\n",
        "print(\"All filenames have been anonymized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-dBYSBAukzw",
        "outputId": "7169fa37-02f0-4504-f963-75a38aaafb3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All filenames have been anonymized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Define input and output paths\n",
        "real_videos_path = \"/content/drive/My Drive/DFD_REAL\"\n",
        "fake_videos_path = \"/content/drive/My Drive/DFD_FAKE\"\n",
        "output_path = \"/content/extracted_frames\"\n",
        "\n",
        "# Ensure the base output path exists\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Process both real and fake videos\n",
        "for category, video_path in [(\"real\", real_videos_path), (\"fake\", fake_videos_path)]:\n",
        "    videos = sorted([f for f in os.listdir(video_path) if f.endswith(\".mp4\")])\n",
        "\n",
        "    for video_file in videos:\n",
        "        video_full_path = os.path.join(video_path, video_file)\n",
        "        video_name = os.path.splitext(video_file)[0]\n",
        "        frame_output_folder = os.path.join(output_path, category, video_name)\n",
        "\n",
        "        os.makedirs(frame_output_folder, exist_ok=True)\n",
        "\n",
        "        cap = cv2.VideoCapture(video_full_path)\n",
        "        frame_count = 0\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            print(f\"ERROR: Could not open {video_file}\")\n",
        "            continue\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_filename = os.path.join(frame_output_folder, f\"frame_{frame_count:04d}.jpg\")\n",
        "            success = cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "            if not success:\n",
        "                print(f\"ERROR: Failed to write frame {frame_count} for {video_file}\")\n",
        "                break  # Stop processing if frame saving fails\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        if frame_count == 0:\n",
        "            print(f\"WARNING: No frames extracted from {video_file}\")\n",
        "        else:\n",
        "            print(f\"Extracted {frame_count} frames from {video_file}.\")\n",
        "\n",
        "print(\"Frame extraction complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5vpgDmqu3xP",
        "outputId": "30f335db-b42f-4dc1-cc6c-c9ef0de6ffbd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 851 frames from video_000.mp4.\n",
            "Extracted 1045 frames from video_001.mp4.\n",
            "Extracted 860 frames from video_002.mp4.\n",
            "Extracted 1085 frames from video_003.mp4.\n",
            "Extracted 306 frames from video_004.mp4.\n",
            "Extracted 800 frames from video_005.mp4.\n",
            "Extracted 834 frames from video_006.mp4.\n",
            "Extracted 305 frames from video_007.mp4.\n",
            "Extracted 560 frames from video_008.mp4.\n",
            "Extracted 610 frames from video_009.mp4.\n",
            "Extracted 787 frames from video_010.mp4.\n",
            "Extracted 626 frames from video_011.mp4.\n",
            "Extracted 902 frames from video_012.mp4.\n",
            "Extracted 371 frames from video_013.mp4.\n",
            "Extracted 965 frames from video_014.mp4.\n",
            "Extracted 1489 frames from video_015.mp4.\n",
            "Extracted 679 frames from video_016.mp4.\n",
            "Extracted 1567 frames from video_017.mp4.\n",
            "Extracted 560 frames from video_018.mp4.\n",
            "Extracted 701 frames from video_019.mp4.\n",
            "Extracted 625 frames from video_020.mp4.\n",
            "Extracted 552 frames from video_021.mp4.\n",
            "Extracted 579 frames from video_022.mp4.\n",
            "Extracted 736 frames from video_023.mp4.\n",
            "Extracted 578 frames from video_024.mp4.\n",
            "Extracted 404 frames from video_025.mp4.\n",
            "Extracted 1550 frames from video_026.mp4.\n",
            "Extracted 641 frames from video_027.mp4.\n",
            "Extracted 856 frames from video_028.mp4.\n",
            "Extracted 734 frames from video_029.mp4.\n",
            "Extracted 700 frames from video_030.mp4.\n",
            "Extracted 580 frames from video_031.mp4.\n",
            "Frame extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "output_path = \"/content/extracted_frames\"\n",
        "\n",
        "for category in [\"real\", \"fake\"]:\n",
        "    category_path = os.path.join(output_path, category)\n",
        "\n",
        "    if not os.path.exists(category_path):\n",
        "        print(f\"ERROR: Path does not exist: {category_path}\")\n",
        "        continue\n",
        "\n",
        "    video_folders = sorted(os.listdir(category_path))\n",
        "    print(f\"Checking {category}: Found {len(video_folders)} subfolders.\")\n",
        "\n",
        "    for video in video_folders[:5]:  # Check first 5 video folders\n",
        "        video_path = os.path.join(category_path, video)\n",
        "        frames = [f for f in os.listdir(video_path) if f.endswith(\".jpg\")]\n",
        "\n",
        "        print(f\"  {video}: {len(frames)} frames\")\n",
        "\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T_JxYVo8113",
        "outputId": "f3f5f05a-c576-4521-e5a2-cf960d30eb9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking real: Found 16 subfolders.\n",
            "  video_000: 851 frames\n",
            "  video_001: 1045 frames\n",
            "  video_002: 860 frames\n",
            "  video_003: 1085 frames\n",
            "  video_004: 306 frames\n",
            "----------------------------------------\n",
            "Checking fake: Found 16 subfolders.\n",
            "  video_016: 679 frames\n",
            "  video_017: 1567 frames\n",
            "  video_018: 560 frames\n",
            "  video_019: 701 frames\n",
            "  video_020: 625 frames\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to extracted frames\n",
        "output_path = \"/content/extracted_frames\"\n",
        "\n",
        "# Find the minimum number of frames across all sequences\n",
        "min_frames = float(\"inf\")\n",
        "\n",
        "for category in [\"real\", \"fake\"]:\n",
        "    category_path = os.path.join(output_path, category)\n",
        "    video_folders = sorted(os.listdir(category_path))\n",
        "\n",
        "    for video in video_folders:\n",
        "        video_path = os.path.join(category_path, video)\n",
        "        frame_count = len([f for f in os.listdir(video_path) if f.endswith(\".jpg\")])\n",
        "\n",
        "        if frame_count > 0:\n",
        "            min_frames = min(min_frames, frame_count)\n",
        "\n",
        "print(f\"Calculated minimum sequence length: {min_frames} frames.\")\n",
        "\n",
        "# Handle case where no valid frames were found\n",
        "if min_frames == float(\"inf\"):\n",
        "    print(\"ERROR: No frames found in dataset. Trimming aborted.\")\n",
        "else:\n",
        "    print(\"Proceeding with trimming.\")\n",
        "\n",
        "    # Trim all sequences to match the shortest sequence\n",
        "    for category in [\"real\", \"fake\"]:\n",
        "        category_path = os.path.join(output_path, category)\n",
        "        video_folders = sorted(os.listdir(category_path))\n",
        "\n",
        "        for video in video_folders:\n",
        "            video_path = os.path.join(category_path, video)\n",
        "            frames = sorted([f for f in os.listdir(video_path) if f.endswith(\".jpg\")])\n",
        "\n",
        "            # Remove excess frames\n",
        "            for frame in frames[min_frames:]:\n",
        "                os.remove(os.path.join(video_path, frame))\n",
        "\n",
        "    print(\"Frame sequences have been standardized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxWdfsKOv9AI",
        "outputId": "184183f0-cda5-4bb4-d679-136a9631f46f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated minimum sequence length: 305 frames.\n",
            "Proceeding with trimming.\n",
            "Frame sequences have been standardized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Pipeline for Deepfake Detection\n",
        "\n",
        "## Why This Architecture?\n",
        "Deepfake detection is challenging because synthetic videos often contain subtle artifacts that are difficult to identify in individual frames. However, a key weakness of deepfake generation is its struggle to accurately replicate natural motion over time.\n",
        "\n",
        "To exploit this weakness, our model will focus on both **spatial inconsistencies (within frames)** and **temporal inconsistencies (across frames)**. This approach ensures we capture unnatural frame transitions, smoothing effects, and interpolation artifacts commonly found in deepfakes.\n",
        "\n",
        "We will achieve this by leveraging **Optical Flow**, **CNNs for spatial analysis**, **LSTMs for temporal modeling**, and **Bayesian Optimization for hyperparameter tuning**.\n",
        "\n",
        "## Final ML Pipeline\n",
        "\n",
        "### Step 1: Extract Optical Flow Features → Capture Motion Anomalies\n",
        "- Optical Flow measures how pixels move between consecutive frames.\n",
        "- Real videos exhibit natural, non-uniform motion, while deepfake videos often show overly smooth, algorithmic motion patterns due to interpolation artifacts.\n",
        "- We compute optical flow vectors and extract motion features to highlight unnatural consistencies between frames.\n",
        "\n",
        "### Step 2: Feed Optical Flow into CNN → Learn Spatial Deepfake Artifacts\n",
        "- We use a pretrained **CNN (ResNet or EfficientNet)** to extract spatial features from the optical flow images.\n",
        "- CNNs can detect face warping, blurring, and inconsistencies in deepfake video frames.\n",
        "- These spatial features act as input to the LSTM, which will analyze their behavior over time.\n",
        "\n",
        "### Step 3: Pass CNN Outputs to LSTM → Learn Temporal Motion Patterns\n",
        "- The **LSTM (Long Short-Term Memory network)** is crucial because it learns how features change across frames.\n",
        "- Real videos exhibit natural, chaotic motion, whereas deepfakes tend to have more consistent and predictable transitions due to synthetic smoothing.\n",
        "- The LSTM helps model these temporal dependencies, allowing the network to recognize unnatural motion transitions.\n",
        "\n",
        "### Step 4: Train Model + Optimize with Bayesian Tuning\n",
        "- **Bayesian Optimization** is used to tune the CNN & LSTM hyperparameters, ensuring:\n",
        "  - **Optimal learning rates** (to improve convergence).\n",
        "  - **Best batch size & sequence length** (to maximize feature extraction).\n",
        "  - **Regularization parameters** (to prevent overfitting).\n",
        "- Bayesian methods are preferred over grid search because they reduce computation time while finding the best hyperparameters.\n",
        "\n",
        "### Step 5: Evaluate on Real vs. Fake Video Sequences\n",
        "- We test the model using unseen deepfake and real video sequences.\n",
        "- **Performance metrics** will include:\n",
        "  - **Accuracy & AUC-ROC** (overall classification performance).\n",
        "  - **Motion Anomaly Scores** (to measure how unnatural motion patterns contribute to classification).\n",
        "  - **Feature Visualizations** (heatmaps on optical flow to show deepfake artifacts).\n",
        "- If the model performs well, we can experiment with other optimizations, such as **Neural Architecture Search (NAS)** or **Transformer-based temporal modeling**.\n"
      ],
      "metadata": {
        "id": "IyttHnk2rA7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Paths\n",
        "base_path = \"/content/extracted_frames\"\n",
        "train_path = \"/content/train\"\n",
        "test_path = \"/content/test\"\n",
        "holdout_path = \"/content/holdout\"\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(train_path, exist_ok=True)\n",
        "os.makedirs(test_path, exist_ok=True)\n",
        "os.makedirs(holdout_path, exist_ok=True)\n",
        "\n",
        "# Categories\n",
        "categories = [\"real\", \"fake\"]\n",
        "\n",
        "# Step 1: Move one sequence from each category to the holdout set\n",
        "for category in categories:\n",
        "    category_path = os.path.join(base_path, category)\n",
        "    holdout_dest = os.path.join(holdout_path, category)\n",
        "    os.makedirs(holdout_dest, exist_ok=True)\n",
        "\n",
        "    # Check if holdout already contains a file\n",
        "    if os.listdir(holdout_dest):\n",
        "        print(f\"Skipping {category}: Holdout set already contains a video.\")\n",
        "        continue\n",
        "\n",
        "    video_folders = sorted(os.listdir(category_path))\n",
        "\n"
      ],
      "metadata": {
        "id": "ddl8HYKG9djP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DRz87AAVQFGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_path = \"/content/train\"\n",
        "test_path = \"/content/test\"\n",
        "holdout_path = \"/content/holdout\"\n",
        "\n",
        "for category in [\"real\", \"fake\"]:\n",
        "    train_category_path = os.path.join(train_path, category)\n",
        "    test_category_path = os.path.join(test_path, category)\n",
        "    holdout_category_path = os.path.join(holdout_path, category)\n",
        "\n",
        "    train_videos = os.listdir(train_category_path) if os.path.exists(train_category_path) else []\n",
        "    test_videos = os.listdir(test_category_path) if os.path.exists(test_category_path) else []\n",
        "    holdout_videos = os.listdir(holdout_category_path) if os.path.exists(holdout_category_path) else []\n",
        "\n",
        "    print(f\"{category.upper()} - Train: {len(train_videos)} videos, Test: {len(test_videos)} videos, Holdout: {len(holdout_videos)} video(s)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGoXw3Dp9lT-",
        "outputId": "1ec8ed09-4909-489b-ad15-aaea0fffb158"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REAL - Train: 0 videos, Test: 0 videos, Holdout: 0 video(s)\n",
            "FAKE - Train: 0 videos, Test: 0 videos, Holdout: 0 video(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Paths\n",
        "base_path = \"/content/extracted_frames\"\n",
        "train_path = \"/content/train\"\n",
        "test_path = \"/content/test\"\n",
        "holdout_path = \"/content/holdout\"\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(train_path, exist_ok=True)\n",
        "os.makedirs(test_path, exist_ok=True)\n",
        "os.makedirs(holdout_path, exist_ok=True)\n",
        "\n",
        "# Categories\n",
        "categories = [\"real\", \"fake\"]\n",
        "\n",
        "# Step 1: Move one sequence from each category to the holdout set\n",
        "for category in categories:\n",
        "    category_path = os.path.join(base_path, category)\n",
        "    holdout_dest = os.path.join(holdout_path, category)\n",
        "    os.makedirs(holdout_dest, exist_ok=True)\n",
        "\n",
        "    video_folders = sorted(os.listdir(category_path))\n",
        "\n",
        "    if not video_folders:\n",
        "        print(f\"WARNING: No videos found in {category_path}. Skipping holdout selection.\")\n",
        "        continue\n",
        "\n",
        "    holdout_sample = random.choice(video_folders)  # Select one video randomly\n",
        "    shutil.move(os.path.join(category_path, holdout_sample), holdout_dest)\n",
        "\n",
        "    print(f\"Moved {holdout_sample} to holdout set.\")\n",
        "\n",
        "# Step 2: Split the remaining data into train (80%) and test (20%)\n",
        "for category in categories:\n",
        "    category_path = os.path.join(base_path, category)\n",
        "    train_dest = os.path.join(train_path, category)\n",
        "    test_dest = os.path.join(test_path, category)\n",
        "\n",
        "    os.makedirs(train_dest, exist_ok=True)\n",
        "    os.makedirs(test_dest, exist_ok=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob8xztU09QQW",
        "outputId": "c1fde936-c012-4142-cc45-ece68a9f2153"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moved video_003 to holdout set.\n",
            "Moved video_018 to holdout set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Checkpoint: Saving Dataset to Google Drive  \n",
        "\n",
        "Since Colab runtimes **reset after inactivity** or if the session crashes, I am saving a **checkpoint** to Google Drive. This ensures i'm don’t lose:  \n",
        "\n",
        "- **Extracted and standardized frame sequences** (`/content/train`, `/content/test`, `/content/holdout`)  \n",
        "- **Preprocessed data for Optical Flow computation**  \n",
        "\n",
        "This saves time and prevents me from having to redo all preprocessing if the runtime disconnects.  \n",
        "\n",
        "###  Restore Later  \n",
        "If the session resets, **restore the dataset** by running:  \n",
        "\n",
        "```python\n",
        "import shutil\n",
        "\n",
        "# Restore from Google Drive\n",
        "restore_path = \"/content/drive/My Drive/DFD_checkpoint.zip\"\n",
        "shutil.unpack_archive(restore_path, \"/content\")\n",
        "\n",
        "print(\"Checkpoint restored successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TLdy-DzUA4Qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "dataset_folders = [\"/content/train\", \"/content/test\", \"/content/holdout\"]\n",
        "zip_path = \"/content/DFD_checkpoint.zip\"\n",
        "save_path = \"/content/drive/My Drive/DFD_checkpoint.zip\"\n",
        "\n",
        "# Ensure only the dataset folders are zipped\n",
        "shutil.make_archive(\"/content/DFD_checkpoint\", 'zip', root_dir=\"/content\", base_dir=\"train\")\n",
        "shutil.make_archive(\"/content/DFD_checkpoint\", 'zip', root_dir=\"/content\", base_dir=\"test\")\n",
        "shutil.make_archive(\"/content/DFD_checkpoint\", 'zip', root_dir=\"/content\", base_dir=\"holdout\")\n",
        "\n",
        "# Move zip file to Google Drive\n",
        "shutil.move(zip_path, save_path)\n",
        "\n",
        "print(f\"Checkpoint saved to Google Drive at: {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1kfPYnVCkuY",
        "outputId": "5dfd6040-79b3-4aed-b5c6-bd4e42604193"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved to Google Drive at: /content/drive/My Drive/DFD_checkpoint.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class OpticalFlowDataset(Dataset):\n",
        "    def __init__(self, data_path, sequence_length=305, transform=None):\n",
        "        \"\"\"\n",
        "        PyTorch dataset for loading video sequences and computing Optical Flow dynamically.\n",
        "\n",
        "        Args:\n",
        "        - data_path (str): Path to the dataset (train/test folder)\n",
        "        - sequence_length (int): Number of frames per sequence\n",
        "        - transform: Torchvision transformations (optional)\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.sequence_length = sequence_length\n",
        "        self.transform = transform\n",
        "        self.video_folders = sorted(os.listdir(data_path))  # Ensure correct order\n",
        "\n",
        "    def compute_optical_flow(self, prev_frame, next_frame):\n",
        "        \"\"\"\n",
        "        Compute Optical Flow using Farneback method.\n",
        "        Returns a grayscale magnitude map highlighting motion inconsistencies.\n",
        "        \"\"\"\n",
        "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "        next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None,\n",
        "                                            0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "        mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "        mag = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
        "        return mag.astype(np.uint8)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_folders)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Load a sequence of frames, compute Optical Flow maps, and return a processed tensor.\n",
        "        \"\"\"\n",
        "        video_folder = self.video_folders[idx]\n",
        "        video_path = os.path.join(self.data_path, video_folder)\n",
        "\n",
        "        # Get frame file names in correct order\n",
        "        frame_files = sorted([f for f in os.listdir(video_path) if f.endswith(\".jpg\")])\n",
        "\n",
        "        # Ensure the sequence length is correct\n",
        "        if len(frame_files) < self.sequence_length:\n",
        "            raise ValueError(f\"Video {video_folder} has fewer frames ({len(frame_files)}) than expected {self.sequence_length}\")\n",
        "\n",
        "        # Load and process Optical Flow between consecutive frames\n",
        "        optical_flow_sequence = []\n",
        "        for i in range(self.sequence_length - 1):  # -1 because we're computing flow between pairs\n",
        "            frame1 = cv2.imread(os.path.join(video_path, frame_files[i]))\n",
        "            frame2 = cv2.imread(os.path.join(video_path, frame_files[i + 1]))\n",
        "\n",
        "            flow_map = self.compute_optical_flow(frame1, frame2)\n",
        "\n",
        "            if self.transform:\n",
        "                flow_map = self.transform(flow_map)\n",
        "\n",
        "            optical_flow_sequence.append(flow_map)\n",
        "\n",
        "        # Convert to Torch tensor (shape: [sequence_length-1, H, W])\n",
        "        optical_flow_sequence = np.array(optical_flow_sequence)\n",
        "        optical_flow_sequence = torch.tensor(optical_flow_sequence, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "        return optical_flow_sequence\n",
        "\n",
        "# Example Usage:\n",
        "train_dataset = OpticalFlowDataset(data_path=\"/content/train/real\", sequence_length=305)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Verify batch shape\n",
        "for batch in train_loader:\n",
        "    print(\"Batch shape:\", batch.shape)  # Expected shape: [batch_size, sequence_length-1, 1, H, W]\n",
        "    break\n"
      ],
      "metadata": {
        "id": "7fVnDBVh_rTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ok ran out of time......"
      ],
      "metadata": {
        "id": "svizS0_gQxDQ"
      }
    }
  ]
}